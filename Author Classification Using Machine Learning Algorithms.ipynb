{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text-based author classification on the Reuter Corpus C50 dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem Description**\n",
    "\n",
    "In Reuter C50 dataset, my work has been focused on author classification. I trained several machine learning algorithms using the C50 train dataset, evaluated the model using the C50 test dataset and compared the different models based on their test set classification accuracies. \n",
    "\n",
    "The model is intended to classify authors based on linguistic analysis of the documents using TF-IDF matrices. TfidfVectorizer transforms the preprocessed list of words into numerical representation, inflating the importance of terms that are frequent within a document(TF) but are rare across all the documents in the corpus(IDF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kowsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('punkt') # for tokenizing\n",
    "\n",
    "# Helper function to read data from a labeled directory\n",
    "def read_data_from_directory(directory_path):\n",
    "    authors = os.listdir(directory_path)\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for author in authors:\n",
    "        author_path = os.path.join(directory_path, author)\n",
    "        for file_name in os.listdir(author_path):\n",
    "            file_path = os.path.join(author_path, file_name)\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                texts.append(f.read())\n",
    "                labels.append(author)\n",
    "    return texts, labels\n",
    "\n",
    "# Reading the training and test data\n",
    "train_texts, train_labels = read_data_from_directory(\"C:/Users/kowsh/OneDrive/Documents/MSBA Coursework/ISLR by Prof James/STA380-master/STA380-master/data/ReutersC50/C50train\")\n",
    "test_texts, test_labels = read_data_from_directory(\"C:/Users/kowsh/OneDrive/Documents/MSBA Coursework/ISLR by Prof James/STA380-master/STA380-master/data/ReutersC50/C50test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Internet may be overflowing with new technology but crime in cyberspace is still of the old-fashioned variety.\\nThe National Consumers League said Wednesday that the most popular scam on the Internet was the pyramid scheme, in which early investors in a bogus fund are paid off with deposits of later investors.\\nThe league, a non-profit consumer advocacy group, tracks web scams through a site it set up on the world wide web in February called Internet Fraud Watch at http://www.fraud.org.\\nThe site, which collects reports directly from consumers, has been widely praised by law enforcement agencies.\\n\"Consumers who suspect a scam on the Internet have critical information,\" said Jodie Bernstein, director of the Federal Trade Commission\\'s Bureau of Consumer Protection. Internet Fraud Watch \"has been a major help to the FTC in identifying particular scams in their infancy.\"\\nIn May, for example, the commission used Internet reports to shut down a site run by Fortuna Alliance that had taken in over $6 million, promising investors they could earn $5,000 a month from an initial deposit of $250. Instead, Fortuna kept most of the money, the commission charged.\\nFraud reports from the league\\'s site, which has been visited over 370,000 times, are forwarded to local, state and federal authorities.\\nThe second-most-popular Internet scam, the league said, was the sale of bogus Internet services, such as custom designed web sites or Internet access accounts.\\nIn third place were crooks who sell computer equipment, such as memory chips or sound boards, over the net and then deliver significantly lower quality goods or nothing at all, the league said.\\nOther top scams involve business opportunities. Con artists may offer shares in a business or franchise using unreasonable predictions or misrepresentations. One popular scheme promised to let consumers get rich while working at home.\\nThe League also announced Tuesday that NationsBank had donated $100,000 to become a sponsor of the Fraud Watch site.\\n'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kowsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Helper function to tokenize, remove punctuation, and stem\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove punctuation\n",
    "    tokens = [token for token in tokens if token not in string.punctuation and token not in stop_words]\n",
    "    \n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    return stemmed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of training documents: 2500\n",
      "Total number of test documents: 2500\n",
      "\n",
      "Average document length in training set: 328.4308 words\n",
      "Minimum document length in training set: 33 words\n",
      "Maximum document length in training set: 957 words\n",
      "\n",
      "Average document length in test set: 333.2564 words\n",
      "Minimum document length in test set: 42 words\n",
      "Maximum document length in test set: 934 words\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Basic Statistics\n",
    "print(f\"Total number of training documents: {len(train_texts)}\")\n",
    "print(f\"Total number of test documents: {len(test_texts)}\")\n",
    "\n",
    "train_doc_lengths = [len(preprocess_text(text)) for text in train_texts]\n",
    "test_doc_lengths = [len(preprocess_text(text)) for text in test_texts]\n",
    "\n",
    "print(f\"\\nAverage document length in training set: {np.mean(train_doc_lengths)} words\")\n",
    "print(f\"Minimum document length in training set: {np.min(train_doc_lengths)} words\")\n",
    "print(f\"Maximum document length in training set: {np.max(train_doc_lengths)} words\")\n",
    "print(f\"\\nAverage document length in test set: {np.mean(test_doc_lengths)} words\")\n",
    "print(f\"Minimum document length in test set: {np.min(test_doc_lengths)} words\")\n",
    "print(f\"Maximum document length in test set: {np.max(test_doc_lengths)} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most common words across the training set:\n",
      "said: 19856\n",
      "'s: 14872\n",
      "'': 13882\n",
      "``: 13712\n",
      "year: 6146\n",
      "compani: 5651\n",
      "would: 5237\n",
      "percent: 5211\n",
      "million: 4942\n",
      "market: 4629\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA/wAAAIhCAYAAADzdrG+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABZTUlEQVR4nO3de3zP9f//8ft723snzWYOOzBDkdMcQkQx5xw+kgqhKFE5ZCQllVEOKfKJ9OnTx8ekFp/6IKlkxKSIRA7JR+VUNitmG1uz7f36/dF37593G/be3tt7e3W7Xi678Hq+nu/n6/F6e5rdPV+v19tiGIYhAAAAAABgKh7uLgAAAAAAALgegR8AAAAAABMi8AMAAAAAYEIEfgAAAAAATIjADwAAAACACRH4AQAAAAAwIQI/AAAAAAAmROAHAAAAAMCECPwAAAAAAJgQgR8A4FJxcXGyWCz2L19fX4WGhqpz586aM2eOUlJS3F1iubZkyRLFxcUVuX+dOnXUt2/f0iuohOLj47Vw4cIC7cePH5fFYtHLL79crHG3bt3qMM+8vb1VvXp1dejQQdOmTdOJEycKvCZ/bh4/ftypY82ePVtr16516jWFHSs6OlpNmzZ1apxr+fjjjxUbG1vovjp16mjEiBEuPR4AoGIh8AMASsWyZcu0Y8cOJSQk6LXXXlOLFi304osvqlGjRtq0aZO7yyu3nA385d2VAr+rzJ49Wzt27NCWLVu0dOlSRUdH69///rcaNWqkd955x6Fvnz59tGPHDoWFhTl9DGcDf3GP5ayPP/5YM2bMKHTfmjVr9Oyzz5bq8QEA5ZuXuwsAAJhT06ZN1bp1a/v2XXfdpYkTJ+rWW2/VgAEDdPToUYWEhLixQphB/fr11a5dO/t2v3799Pjjj6tbt24aMWKEmjVrpqioKElS9erVVb169VKtJysrS76+vmVyrGtp2bKlW48PAHA/VvgBAGWmdu3amj9/vjIyMvTGG2847Fu3bp1uueUW+fv7KyAgQN27d9eOHTsKjPH999/r3nvvVUhIiHx8fFS7dm3df//9ys7OliTFxsbKYrEUeF1hl1jnXw6/fv16tWzZUn5+fmrUqJHWr19vf02jRo1UqVIl3Xzzzfr6668LjPv111+rX79+Cg4Olq+vr1q2bKn//Oc/hR57y5YtevTRR1WtWjVVrVpVAwYM0OnTpx3qOXTokBITE+2XqtepU6fI7++VGIahJUuWqEWLFvLz81OVKlV0991366effnLol3/J+e7du3XbbbfJ399f9erV09y5c2Wz2Rz6Hjp0SD169JC/v7+qV6+usWPH6qOPPpLFYtHWrVvt43300Uc6ceKEw+X3f7ZgwQLVrVtX1113nW655Rbt3LmzROcbHBysN954Q7m5uXrllVfs7YXNgb1796pv376qUaOGfHx8FB4erj59+ujnn3+WJFksFl28eFHLly+31x8dHe0w3saNG/Xggw+qevXq8vf3V3Z29lVvH/j888/Vrl07+fn5qWbNmnr22WeVl5dn359/u0L++5gv/zaI/CtARowYoddee81eZ/5X/jELu6T/5MmTGjZsmP18GzVqpPnz5zv8+V5+u4Wr/2wAAGWLwA8AKFO9e/eWp6entm3bZm+Lj4/XHXfcocqVK+vdd9/V0qVLlZqaqujoaG3fvt3e79tvv1WbNm20c+dOzZw5U5988onmzJmj7OxsXbp0qVj1fPvtt5o6daqefPJJrV69WoGBgRowYICmT5+uf/3rX5o9e7beeecdpaWlqW/fvsrKyrK/dsuWLerQoYPOnz+vf/zjH/rggw/UokULDRo0qNDL8h966CFZrVbFx8dr3rx52rp1q4YNG2bfv2bNGtWrV08tW7bUjh07tGPHDq1Zs6ZY53W5hx9+WDExMerWrZvWrl2rJUuW6NChQ2rfvr3OnDnj0Dc5OVlDhw7VsGHDtG7dOvXq1UtTp07V22+/be+TlJSkTp066ciRI3r99df11ltvKSMjQ+PGjXMYa8mSJerQoYNCQ0Pt5/Pn/8R57bXXlJCQoIULF+qdd97RxYsX1bt3b6WlpZXonNu0aaOwsDCHefZnFy9eVPfu3XXmzBmHOmrXrq2MjAxJ0o4dO+Tn56fevXvb61+yZInDOA8++KCsVqtWrFih999/X1ar9YrHTE5O1uDBgzV06FB98MEHuvvuu/XCCy9owoQJTp/js88+q7vvvtteZ/7XlW4j+PXXX9W+fXtt3LhRzz//vNatW6du3bpp8uTJBf7spNL7swEAlCEDAAAXWrZsmSHJ2L179xX7hISEGI0aNTIMwzDy8vKM8PBwIyoqysjLy7P3ycjIMGrUqGG0b9/e3talSxcjKCjISElJueLY06dPNwr75y2/rmPHjtnbIiMjDT8/P+Pnn3+2t+3bt8+QZISFhRkXL160t69du9aQZKxbt87e1rBhQ6Nly5ZGTk6Ow7H69u1rhIWF2c8n/9hjxoxx6Ddv3jxDkpGUlGRva9KkidGpU6crnt+fRUZGGn369Lni/h07dhiSjPnz5zu0nzp1yvDz8zOmTJlib+vUqZMhyfjqq68c+jZu3Njo2bOnffuJJ54wLBaLcejQIYd+PXv2NCQZW7Zssbf16dPHiIyMLFDXsWPHDElGVFSUkZuba2/ftWuXIcl49913r3reW7ZsMSQZ77333hX7tG3b1vDz87Nv/3kOfP3114YkY+3atVc9VqVKlYzhw4cXaM8f7/7777/ivsvnW/77+8EHHzj0HTVqlOHh4WGcOHHC4dwufx8N4/+/Z8uWLbO3jR07ttD5bhh/zI3L637qqacK/fN99NFHDYvFYhw5csThOMX9swEAlB+s8AMAypxhGPbfHzlyRKdPn9Z9990nD4///8/Sddddp7vuuks7d+5UZmamMjMzlZiYqIEDB7r03ugWLVqoZs2a9u1GjRpJ+uNydH9//wLt+U9//+GHH/T9999r6NChkqTc3Fz7V+/evZWUlKQjR444HKtfv34O282aNXMYszSsX79eFotFw4YNc6gxNDRUzZs3L3DZeGhoqG6++eYCdV5eY2Jiopo2barGjRs79Lv33nudrq9Pnz7y9PR0OJbkmvfk8nlWmBtuuEFVqlTRk08+qX/84x/67rvvinWcu+66q8h9AwICCsyDIUOGyGazXfVqBFf47LPP1Lhx4wJ/viNGjJBhGPrss88c2kvzzwYAUDYI/ACAMnXx4kWdPXtW4eHhkqSzZ89KUqGXIYeHh8tmsyk1NVWpqanKy8tTrVq1XFpPcHCww7a3t/dV23///XdJsl8KP3nyZFmtVoevMWPGSJJ+++03hzGqVq3qsO3j4yNJDrcJuNqZM2dkGIZCQkIK1Llz585r1phf5+U1nj17ttAHLhbnIYyl+Z6cPHnSPs8KExgYqMTERLVo0UJPP/20mjRpovDwcE2fPl05OTlFPo4zT+Iv7D0KDQ2V9P//LpSWs2fPXvHvWWHHd8d8BQC4Fk/pBwCUqY8++kh5eXn2B5/lh4qkpKQCfU+fPi0PDw9VqVJFFotFnp6e9oepXYmvr68kKTs72x5QpILhu6SqVasmSZo6daoGDBhQaJ8bb7zRpccsjmrVqslisejzzz93eD/yFdZ2LVWrVi1w77/0x/3p5cWuXbuUnJyskSNHXrVfVFSUVq5cKcMwtH//fsXFxWnmzJny8/PTU089VaRjFfYgwiu52vuW/3fh8jl8uZLO4apVq17x75n0/+c0AMA8WOEHAJSZkydPavLkyQoMDNTDDz8s6Y9QXLNmTcXHxztcgn3x4kX997//tT+538/PT506ddJ777131eCT/1T7/fv3O7R/+OGHLj2XG2+8UfXr19e3336r1q1bF/oVEBDg9Lh/Xk0vqb59+8owDP3yyy+F1pj/kXXO6NSpkw4ePFjgEviVK1cW6Ovq8ymKc+fO6ZFHHpHVatXEiROL9BqLxaLmzZvrlVdeUVBQkL755hv7PleeQ0ZGhtatW+fQFh8fLw8PD3Xs2FHSlefwn1+XX5tUtFX3rl276rvvvnM4N0l66623ZLFY1Llz5yKfBwCgYmCFHwBQKg4ePGi/XzwlJUWff/65li1bJk9PT61Zs8Z+H76Hh4fmzZunoUOHqm/fvnr44YeVnZ2tl156SefPn9fcuXPtYy5YsEC33nqr2rZtq6eeeko33HCDzpw5o3Xr1umNN95QQECAevfureDgYI0cOVIzZ86Ul5eX4uLidOrUKZef4xtvvKFevXqpZ8+eGjFihGrWrKlz587p8OHD+uabb/Tee+85PWb+ivOqVatUr149+fr6XjOUJycn6/333y/QXqdOHXXo0EGjR4/WAw88oK+//lodO3ZUpUqVlJSUpO3btysqKkqPPvqoUzXGxMTo3//+t3r16qWZM2cqJCRE8fHx+v777yXJ4VkMUVFRWr16tV5//XW1atVKHh4eat26tVPHu5qjR49q586dstlsOnv2rL766istXbpU6enpeuutt9SkSZMrvnb9+vVasmSJ+vfvr3r16skwDK1evVrnz59X9+7dHc5h69at+vDDDxUWFqaAgIBiX71RtWpVPfroozp58qQaNGigjz/+WG+++aYeffRR1a5dW9Ifl/h369ZNc+bMUZUqVRQZGanNmzdr9erVBcbLnxsvvviievXqJU9PTzVr1sx+C8rlJk6cqLfeekt9+vTRzJkzFRkZqY8++khLlizRo48+qgYNGhTrnAAA5ReBHwBQKh544AFJf9z7HhQUpEaNGunJJ5/UQw89VOChe0OGDFGlSpU0Z84cDRo0SJ6enmrXrp22bNmi9u3b2/s1b95cu3bt0vTp0zV16lRlZGQoNDRUXbp0sQecypUra8OGDYqJidGwYcMUFBSkhx56SL169dJDDz3k0nPs3Lmzdu3apVmzZikmJkapqamqWrWqGjdurIEDBxZrzBkzZigpKUmjRo1SRkaGIiMjC/0s98vt2bNH99xzT4H24cOHKy4uTm+88YbatWunN954Q0uWLJHNZlN4eLg6dOhQ4AFuRREeHq7ExETFxMTokUcekb+/v+68807NnDlTw4cPV1BQkL3vhAkTdOjQIT399NNKS0uTYRjXfJieM55++mlJkpeXlwIDA9WgQQM9+OCDGj16tCIjI6/62vr16ysoKEjz5s3T6dOn5e3trRtvvFFxcXEaPny4vd/f//53jR07VoMHD1ZmZqY6depU4GGHRRUaGqrXXntNkydP1oEDBxQcHKynn35aM2bMcOi3YsUKjR8/Xk8++aTy8vL0t7/9Te+++26B/ywZMmSIvvjiCy1ZskQzZ86UYRg6duyY/SqBy1WvXl1ffvmlpk6dqqlTpyo9PV316tXTvHnzNGnSpGKdDwCgfLMYrvxXFwAA/GWNHj1a7777rs6ePVvoCjMAAChbrPADAACnzZw5U+Hh4apXr54uXLig9evX61//+peeeeYZwj4AAOUEgR8AADjNarXqpZde0s8//6zc3FzVr19fCxYs0IQJE9xdGgAA+D9c0g8AAAAAgAnxsXwAAAAAAJgQgR8AAAAAABMi8AMAAAAAYEI8tE+SzWbT6dOnFRAQIIvF4u5yAAAAAAAmZxiGMjIyFB4eLg+P0lmLJ/BLOn36tCIiItxdBgAAAADgL+bUqVOqVatWqYxN4JcUEBAg6Y83unLlytfsn5OTo40bN6pHjx6yWq2lXR5QZpjbMCvmNsyKuQ2zYm7DrC6f21lZWYqIiLDn0dJA4Jfsl/FXrly5yIHf399flStX5hsQTIW5DbNibsOsmNswK+Y2zKqwuV2at5Xz0D4AAAAAAEyIwA8AAAAAgAkR+AEAAAAAMCHu4QcAAAAAODAMQ7m5ucrLy3N3KRWWp6envLy83PrR7wR+AAAAAIDdpUuXlJSUpMzMTHeXUuH5+/srLCxM3t7ebjk+gR8AAAAAIEmy2Ww6duyYPD09FR4eLm9vb7euUFdUhmHo0qVL+vXXX3Xs2DHVr19fHh5lf0c9gR8AAAAAIOmP1X2bzaaIiAj5+/u7u5wKzc/PT1arVSdOnNClS5fk6+tb5jXw0D4AAAAAgAN3rEabkbvfR/4UAQAAAAAwIQI/AAAAAAAmxD38AAAAAIBrGhm3u0yPt3REmzI9XmGio6PVokULLVy40N2lFAuBHwAAAABQoV3rkwSGDx+uuLg4p8ddvXq1rFZrMatyPwI/AAAAAKBCS0pKsv9+1apVeu6553TkyBF7m5+fn0P/nJycIgX54OBg1xXpBtzDDwAAAACo0EJDQ+1fgYGBslgs9u3ff/9dQUFB+s9//qPo6Gj5+vrq7bff1tmzZ3XvvfeqVq1a8vf3V1RUlN59912HcaOjoxUTE2PfrlOnjmbPnq0HH3xQAQEBql27tv75z3+W8dkWHYEfAAAAAGB6Tz75pB577DEdPnxYPXv21O+//65WrVpp/fr1OnjwoEaPHq377rtPX3311VXHmT9/vlq3bq29e/dqzJgxevTRR/X999+X0Vk4h0v6AQAAAACmFxMTowEDBji0TZ482f778ePHa8OGDXrvvffUtm3bK47Tu3dvjRkzRtIf/4nwyiuvaOvWrWrYsGHpFF4CBH4AAAAAgOm1bt3aYTsvL09z587VqlWr9Msvvyg7O1vZ2dmqVKnSVcdp1qyZ/ff5tw6kpKSUSs0lReAHAAAAAJjen4P8/Pnz9corr2jhwoWKiopSpUqVFBMTo0uXLl11nD8/7M9ischms7m8Xlcg8AMAAAAA/nI+//xz3XHHHRo2bJgkyWaz6ejRo2rUqJGbK3MdAj8A/AWMjNtd4jGWjmjjgkoAAADKhxtuuEH//e9/9eWXX6pKlSpasGCBkpOTCfwAAAAAgL8Ws/3n/7PPPqtjx46pZ8+e8vf31+jRo9W/f3+lpaW5uzSXIfADAAAAAExjxIgRGjFihH27Tp06MgyjQL/g4GCtXbv2qmNt3brVYfv48eMF+uzbt8/5IsuIhzsPPmfOHLVp00YBAQGqUaOG+vfvryNHjjj0GTFihCwWi8NXu3btHPpkZ2dr/PjxqlatmipVqqR+/frp559/LstTAQAAAACgXHFr4E9MTNTYsWO1c+dOJSQkKDc3Vz169NDFixcd+t1+++1KSkqyf3388ccO+2NiYrRmzRqtXLlS27dv14ULF9S3b1/l5eWV5ekAAAAAAFBuuPWS/g0bNjhsL1u2TDVq1NCePXvUsWNHe7uPj49CQ0MLHSMtLU1Lly7VihUr1K1bN0nS22+/rYiICG3atEk9e/YsvRMAAAAAAKCcKlf38Oc/HCE4ONihfevWrapRo4aCgoLUqVMnzZo1SzVq1JAk7dmzRzk5OerRo4e9f3h4uJo2baovv/yy0MCfnZ2t7Oxs+3Z6erokKScnRzk5OdesM79PUfoCFQlz27y8VPLPhq3I84K5DbNibsOsmNvuk5OTI8MwZLPZyu1ny1ckNptNhmEoJydHnp6eDnO7LOa3xSjs6QVuYBiG7rjjDqWmpurzzz+3t69atUrXXXedIiMjdezYMT377LPKzc3Vnj175OPjo/j4eD3wwAMOAV6SevToobp16+qNN94ocKzY2FjNmDGjQHt8fLz8/f1df3IAAAAAUAF4eXkpNDRUERER8vb2dnc5Fd6lS5d06tQpJScnKzc312FfZmamhgwZorS0NFWuXLlUjl9uVvjHjRun/fv3a/v27Q7tgwYNsv++adOmat26tSIjI/XRRx9pwIABVxzPMAxZLJZC902dOlWTJk2yb6enpysiIkI9evQo0hudk5OjhIQEde/eXVar9Zr9gYqCuW1e4975psRjLB56kwsqcQ/mNsyKuQ2zYm67z++//65Tp07puuuuk6+vr7vLqfB+//13+fn5qWPHjvL19XWY21lZWaV+/HIR+MePH69169Zp27ZtqlWr1lX7hoWFKTIyUkePHpUkhYaG6tKlS0pNTVWVKlXs/VJSUtS+fftCx/Dx8ZGPj0+BdqvV6tQ3FGf7AxUFc9t8cl3wjFYzzAnmNsyKuQ2zYm6Xvby8PFksFnl4eMjDw63PeDcFDw8PWSyWAnPZarUWWPEvleOX+hGuwjAMjRs3TqtXr9Znn32munXrXvM1Z8+e1alTpxQWFiZJatWqlaxWqxISEux9kpKSdPDgwSsGfgAAAAAAzM6tK/xjx45VfHy8PvjgAwUEBCg5OVmSFBgYKD8/P124cEGxsbG66667FBYWpuPHj+vpp59WtWrVdOedd9r7jhw5Uo8//riqVq2q4OBgTZ48WVFRUfan9gMAAAAA8Ffj1sD/+uuvS5Kio6Md2pctW6YRI0bI09NTBw4c0FtvvaXz588rLCxMnTt31qpVqxQQEGDv/8orr8jLy0sDBw5UVlaWunbtqri4OHl6epbl6QAAAACAecUPunYfVxqyqmyPZ0JuDfzX+oAAPz8/ffrpp9ccx9fXV4sWLdKiRYtcVRoAAAAAoIK40gPb8w0fPlxxcXHFGrtOnTqKiYlRTExMsV7vTuXioX0AAAAAABRXUlKS/ferVq3Sc889pyNHjtjb/Pz83FGW2/HYRQAAAABAhRYaGmr/CgwMlMVicWjbtm2bWrVqJV9fX9WrV08zZsxweEp+bGysateuLR8fH4WHh+uxxx6T9Mft5ydOnNDEiRNlsViueSVBecMKPwAAAADAtD799FMNGzZMr776qm677Tb9+OOPGj16tCRp+vTpev/99/XKK69o5cqVatKkiZKTk/Xtt99KklavXq3mzZtr9OjRGjVqlDtPo1gI/ACAIhkZt7vEYywd0cYFlQAAABTdrFmz9NRTT2n48OGSpHr16un555/XlClTNH36dJ08eVKhoaHq1q2brFarateurZtvvlmSFBwcLE9PTwUEBCg0NNSdp1EsXNIPAAAAADCtPXv2aObMmbruuuvsX6NGjVJSUpIyMzN1zz33KCsrS/Xq1dOoUaO0Zs0ah8v9KzJW+AEAAAAApmWz2TRjxgwNGDCgwD5fX19FREToyJEjSkhI0KZNmzRmzBi99NJLSkxMlNVqdUPFrkPgBwAAAACY1k033aQjR47ohhtuuGIfPz8/9evXT/369dPYsWPVsGFDHThwQDfddJO8vb2Vl5dXhhW7DoEfAMwufpDGnzlf7JcvCnnBdbUAAACUseeee059+/ZVRESE7rnnHnl4eGj//v06cOCAXnjhBcXFxSkvL09t27aVv7+/VqxYIT8/P0VGRkqS6tSpo23btmnw4MHy8fFRtWrV3HxGRUfgBwAAAABc25BV7q6gWHr27Kn169dr5syZmjdvnqxWqxo2bKiHHnpIkhQUFKS5c+dq0qRJysvLU1RUlD788ENVrVpVkjRz5kw9/PDDuv7665WdnS3DMNx5Ok4h8AMAAAAATGPEiBEaMWKEQ1vPnj3Vs2fPQvv3799f/fv3v+J47dq1s39MX0XDU/oBAAAAADAhAj8AAAAAACZE4AcAAAAAwIQI/AAAAAAAmBCBHwAAAADgoCI9ib48c/f7SOAHAAAAAEiSrFarJCkzM9PNlZhD/vuY/76WNT6WDwAAAAAgSfL09FRQUJBSUlIkSf7+/rJYLG6uquIxDEOZmZlKSUlRUFCQPD093VIHgR8AAAAAYBcaGipJ9tCP4gsKCrK/n+5A4AcAAAAA2FksFoWFhalGjRrKyclxdzkVltVqddvKfj4CPwAAAACgAE9PT7cHVpQMgR8AyrmRcbtL9PrxZ867phAAAABUKDylHwAAAAAAE2KFHwBQZkp6tcLSEW1cVAkAAID5scIPAAAAAIAJscIPAOVZ/CDuwQcAAECxsMIPAAAAAIAJEfgBAAAAADAhAj8AAAAAACZE4AcAAAAAwIQI/AAAAAAAmBCBHwAAAAAAEyLwAwAAAABgQgR+AAAAAABMiMAPAAAAAIAJEfgBAAAAADAhAj8AAAAAACZE4AcAAAAAwIQI/AAAAAAAmBCBHwAAAAAAEyLwAwAAAABgQgR+AAAAAABMiMAPAAAAAIAJEfgBAAAAADAhL3cXAAAo38afeaZEr18U8oKLKgEAAIAzWOEHAAAAAMCECPwAAAAAAJgQgR8AAAAAABMi8AMAAAAAYEIEfgAAAAAATIjADwAAAACACfGxfABQikbG7S7R68efOe+aQgAAAPCXwwo/AAAAAAAmROAHAAAAAMCECPwAAAAAAJgQgR8AAAAAABMi8AMAAAAAYEIEfgAAAAAATIjADwAAAACACRH4AQAAAAAwIQI/AAAAAAAmROAHAAAAAMCECPwAAAAAAJiQl7sLAACY2/gzz5R4jEUhL7igEgAAgL8WVvgBAAAAADAhAj8AAAAAACZE4AcAAAAAwIQI/AAAAAAAmBCBHwAAAAAAEyLwAwAAAABgQgR+AAAAAABMiMAPAAAAAIAJEfgBAAAAADAhAj8AAAAAACZE4AcAAAAAwIQI/AAAAAAAmJBbA/+cOXPUpk0bBQQEqEaNGurfv7+OHDni0McwDMXGxio8PFx+fn6Kjo7WoUOHHPpkZ2dr/PjxqlatmipVqqR+/frp559/LstTAQAAAACgXHFr4E9MTNTYsWO1c+dOJSQkKDc3Vz169NDFixftfebNm6cFCxZo8eLF2r17t0JDQ9W9e3dlZGTY+8TExGjNmjVauXKltm/frgsXLqhv377Ky8tzx2kBAAAAAOB2Xu48+IYNGxy2ly1bpho1amjPnj3q2LGjDMPQwoULNW3aNA0YMECStHz5coWEhCg+Pl4PP/yw0tLStHTpUq1YsULdunWTJL399tuKiIjQpk2b1LNnzzI/LwAAAAAA3M2tgf/P0tLSJEnBwcGSpGPHjik5OVk9evSw9/Hx8VGnTp305Zdf6uGHH9aePXuUk5Pj0Cc8PFxNmzbVl19+WWjgz87OVnZ2tn07PT1dkpSTk6OcnJxr1pnfpyh9gYqEue16Y3+dUaLX2zysLqqkYvOSTVLx5yZzG2bF3IZZMbdhVpfP7bKY3+Um8BuGoUmTJunWW29V06ZNJUnJycmSpJCQEIe+ISEhOnHihL2Pt7e3qlSpUqBP/uv/bM6cOZoxo+AP4Rs3bpS/v3+Ra05ISChyX6AiYW67UINR7q7AFHrrj+/nH3/8cYnGYW7DrJjbMCvmNswqISFBmZmZpX6cchP4x40bp/3792v79u0F9lksFodtwzAKtP3Z1fpMnTpVkyZNsm+np6crIiJCPXr0UOXKla9Za05OjhISEtS9e3dZray+wTyY2653YOGd7i7BFN6o/qwkafHQm4r1euY2zIq5DbNibsOsLp/bWVlZpX68chH4x48fr3Xr1mnbtm2qVauWvT00NFTSH6v4YWFh9vaUlBT7qn9oaKguXbqk1NRUh1X+lJQUtW/fvtDj+fj4yMfHp0C71Wp16huKs/2BioK57ToeNi5FdIXc/3vGbEnnJXMbZsXchlkxt2FWVqtVubm5pX4ctz6l3zAMjRs3TqtXr9Znn32munXrOuyvW7euQkNDHS7luXTpkhITE+1hvlWrVrJarQ59kpKSdPDgwSsGfgAAAAAAzM6tK/xjx45VfHy8PvjgAwUEBNjvuQ8MDJSfn58sFotiYmI0e/Zs1a9fX/Xr19fs2bPl7++vIUOG2PuOHDlSjz/+uKpWrarg4GBNnjxZUVFR9qf2AwAAAADwV+PWwP/6669LkqKjox3aly1bphEjRkiSpkyZoqysLI0ZM0apqalq27atNm7cqICAAHv/V155RV5eXho4cKCysrLUtWtXxcXFydPTs6xOBQAAAACAcsWtgd8wjGv2sVgsio2NVWxs7BX7+Pr6atGiRVq0aJELqwMAAAAAoOJy6z38AAAAAACgdBD4AQAAAAAwIQI/AAAAAAAmROAHAAAAAMCECPwAAAAAAJgQgR8AAAAAABMi8AMAAAAAYEJe7i4AAMqrkXG7SzzGeBfUAQAAABQHK/wAAAAAAJgQgR8AAAAAABMi8AMAAAAAYEIEfgAAAAAATIjADwAAAACACRH4AQAAAAAwIQI/AAAAAAAmROAHAAAAAMCECPwAAAAAAJgQgR8AAAAAABMi8AMAAAAAYEIEfgAAAAAATIjADwAAAACACRH4AQAAAAAwIQI/AAAAAAAmROAHAAAAAMCECPwAAAAAAJgQgR8AAAAAABMi8AMAAAAAYEIEfgAAAAAATIjADwAAAACACRH4AQAAAAAwIQI/AAAAAAAm5OXuAgAAuJbxZ5754zfxQcUb4J63XVYLAABARVHiFf709HStXbtWhw8fdkU9AAAAAADABZwO/AMHDtTixYslSVlZWWrdurUGDhyoZs2a6b///a/LCwQAAAAAAM5zOvBv27ZNt912myRpzZo1MgxD58+f16uvvqoXXnjB5QUCAAAAAADnOR3409LSFBwcLEnasGGD7rrrLvn7+6tPnz46evSoywsEAAAAAADOczrwR0REaMeOHbp48aI2bNigHj16SJJSU1Pl6+vr8gIBAAAAAIDznH5Kf0xMjIYOHarrrrtOkZGRio6OlvTHpf5RUVGurg8AAAAAABSD04F/zJgxatu2rU6ePKnu3bvLw+OPiwTq1aunWbNmubxAAAAAAADgPKcv6Z85c6YaNWqkO++8U9ddd529vUuXLtq0aZNLiwMAAAAAAMXjdOCfMWOGLly4UKA9MzNTM2bMcElRAAAAAACgZJwO/IZhyGKxFGj/9ttv7U/vBwAAAAAA7lXke/irVKkii8Uii8WiBg0aOIT+vLw8XbhwQY888kipFAkAAAAAAJxT5MC/cOFCGYahBx98UDNmzFBgYKB9n7e3t+rUqaNbbrmlVIoEAAAAAADOKXLgHz58uCSpbt26at++vaxWa6kVBQAAAAAASsbpj+Xr1KmTbDab/ve//yklJUU2m81hf8eOHV1WHAAAAAAAKB6nA//OnTs1ZMgQnThxQoZhOOyzWCzKy8tzWXEAAAAAAKB4nA78jzzyiFq3bq2PPvpIYWFhhT6xHwAAAAAAuJfTgf/o0aN6//33dcMNN5RGPQAAAAAAwAU8nH1B27Zt9cMPP5RGLQAAAAAAwEWcXuEfP368Hn/8cSUnJysqKqrA0/qbNWvmsuIAAAAAAEDxOB3477rrLknSgw8+aG+zWCwyDIOH9gEAAAAAUE44HfiPHTtWGnUAgEuNjNvt7hIAAAAAt3I68EdGRpZGHQAAAAAAwIWcfmifJK1YsUIdOnRQeHi4Tpw4IUlauHChPvjgA5cWBwAAAAAAisfpwP/6669r0qRJ6t27t86fP2+/Zz8oKEgLFy50dX0AAAAAAKAYnA78ixYt0ptvvqlp06bJ09PT3t66dWsdOHDApcUBAAAAAIDiKdZD+1q2bFmg3cfHRxcvXnRJUQBQXow/84y7SwAAAACKxekV/rp162rfvn0F2j/55BM1btzYFTUBAAAAAIAScnqF/4knntDYsWP1+++/yzAM7dq1S++++67mzJmjf/3rX6VRIwAAAAAAcJLTgf+BBx5Qbm6upkyZoszMTA0ZMkQ1a9bU3//+dw0ePLg0agQAAAAAAE5yOvBL0qhRozRq1Cj99ttvstlsqlGjhqvrAgAAAAAAJVCswJ+vWrVqrqoDAAAAAAC4kNOB/+zZs3ruuee0ZcsWpaSkyGazOew/d+6cy4oDAAAAAADF43TgHzZsmH788UeNHDlSISEhslgspVEXAAAAAAAoAacD//bt27V9+3Y1b968NOoBAAAAAAAu4OHsCxo2bKisrKzSqAUAAAAAALiI04F/yZIlmjZtmhITE3X27Fmlp6c7fAEAAAAAAPdz+pL+oKAgpaWlqUuXLg7thmHIYrEoLy/PZcUBQEmMP/OMu0sAAAAA3MbpwD906FB5e3srPj6eh/YBAAAAAFBOOR34Dx48qL179+rGG28sjXoAAAAAAIALOH0Pf+vWrXXq1KnSqAUAAAAAALiI0yv848eP14QJE/TEE08oKipKVqvVYX+zZs1cVhwAAAAAACgepwP/oEGDJEkPPvigvc1isfDQPgAAAAAAyhGnA/+xY8dKow4AAAAAAOBCTgf+yMjI0qgDAAAAAAC4kNOB/6233rrq/vvvv7/IY23btk0vvfSS9uzZo6SkJK1Zs0b9+/e37x8xYoSWL1/u8Jq2bdtq586d9u3s7GxNnjxZ7777rrKystS1a1ctWbJEtWrVKnIdAAAAAACYjdOBf8KECQ7bOTk5yszMlLe3t/z9/Z0K/BcvXlTz5s31wAMP6K677iq0z+23365ly5bZt729vR32x8TE6MMPP9TKlStVtWpVPf744+rbt6/27NkjT09PJ84MAAAAAADzcDrwp6amFmg7evSoHn30UT3xxBNOjdWrVy/16tXrqn18fHwUGhpa6L60tDQtXbpUK1asULdu3SRJb7/9tiIiIrRp0yb17NnTqXoAAAAAADALpwN/YerXr6+5c+dq2LBh+v77710xpN3WrVtVo0YNBQUFqVOnTpo1a5Zq1KghSdqzZ49ycnLUo0cPe//w8HA1bdpUX3755RUDf3Z2trKzs+3b6enpkv64WiEnJ+eaNeX3KUpfoCIx09z2kk02D+u1O6JCySnmP1tmmtvA5ZjbMCvmNszq8rldFvPbJYFfkjw9PXX69GlXDSfpjysA7rnnHkVGRurYsWN69tln1aVLF+3Zs0c+Pj5KTk6Wt7e3qlSp4vC6kJAQJScnX3HcOXPmaMaMGQXaN27cKH9//yLXl5CQUPSTASoQM8zt3lWkU1VGubsMuNip4r7w/+a0GeY2UBjmNsyKuQ2zSkhIUGZmZqkfx+nAv27dOodtwzCUlJSkxYsXq0OHDi4rTJIGDRpk/33Tpk3VunVrRUZG6qOPPtKAAQOu+DrDMGSxWK64f+rUqZo0aZJ9Oz09XREREerRo4cqV658zbpycnKUkJCg7t27y2plBRHmUV7m9rh3vnHJOA//+rxLxkH5EVUzsFivy+n/ZrmY24CrlZfv24CrMbdhVpfP7aysrFI/ntOB//Kn6EuSxWJR9erV1aVLF82fP99VdRUqLCxMkZGROnr0qCQpNDRUly5dUmpqqsMqf0pKitq3b3/FcXx8fOTj41Og3Wq1OvUNxdn+QEXh7rmdKw+XjONh4zJAs7Eqt5gvtP7fL3zfhjkxt2FWzG2YldVqVW5uMX+ucYLTgd9ms5VGHUVy9uxZnTp1SmFhYZKkVq1ayWq1KiEhQQMHDpQkJSUl6eDBg5o3b57b6gQAAAAAwN1cdg9/cVy4cEE//PCDffvYsWPat2+fgoODFRwcrNjYWN11110KCwvT8ePH9fTTT6tatWq68847JUmBgYEaOXKkHn/8cVWtWlXBwcGaPHmyoqKi7E/tBwAAAADgr8jp62bvvvtuzZ07t0D7Sy+9pHvuucepsb7++mu1bNlSLVu2lCRNmjRJLVu21HPPPSdPT08dOHBAd9xxhxo0aKDhw4erQYMG2rFjhwICAuxjvPLKK+rfv78GDhyoDh06yN/fXx9++KE8PT2dPTUAAAAAAEzD6RX+xMRETZ8+vUD77bffrpdfftmpsaKjo2UYxhX3f/rpp9ccw9fXV4sWLdKiRYucOjYAAAAAAGbm9Ar/hQsX5O3tXaDdarXaP88eAAAAAAC4l9OBv2nTplq1alWB9pUrV6px48YuKQoAAAAAAJSM05f0P/vss7rrrrv0448/qkuXLpKkzZs3691339V7773n8gIBAAAAAIDznA78/fr109q1azV79my9//778vPzU7NmzbRp0yZ16tSpNGoEAAAAAABOKtbH8vXp00d9+vRxdS0AAAAAAMBFihX4JWnPnj06fPiwLBaLGjdubP9oPQAASsu+U+eL9brX3/lGvatI4975Rm+MaOvaogAAAMoppwN/SkqKBg8erK1btyooKEiGYSgtLU2dO3fWypUrVb169dKoEwAAAAAAOMHpp/SPHz9e6enpOnTokM6dO6fU1FQdPHhQ6enpeuyxx0qjRgAAAAAA4CSnV/g3bNigTZs2qVGjRva2xo0b67XXXlOPHj1cWhwAAAAAACgep1f4bTabrFZrgXar1SqbzeaSogAAAAAAQMk4Hfi7dOmiCRMm6PTp0/a2X375RRMnTlTXrl1dWhwAAAAAACgepwP/4sWLlZGRoTp16uj666/XDTfcoLp16yojI0OLFi0qjRoBAAAAAICTnL6HPyIiQt98840SEhL0/fffyzAMNW7cWN26dSuN+gAAAAAAQDE4Hfjzde/eXd27d3dlLQAAAAAAwEWcCvw2m01xcXFavXq1jh8/LovForp16+ruu+/WfffdJ4vFUlp1AgAAAAAAJxT5Hn7DMNSvXz899NBD+uWXXxQVFaUmTZroxIkTGjFihO68887SrBMAAAAAADihyCv8cXFx2rZtmzZv3qzOnTs77Pvss8/Uv39/vfXWW7r//vtdXiQAAAAAAHBOkVf43333XT399NMFwr70x0f1PfXUU3rnnXdcWhwAAAAAACieIgf+/fv36/bbb7/i/l69eunbb791SVEAAAAAAKBkihz4z507p5CQkCvuDwkJUWpqqkuKAgAAAAAAJVPkwJ+Xlycvryvf8u/p6anc3FyXFAUAAAAAAEqmyA/tMwxDI0aMkI+PT6H7s7OzXVYUAAAAAAAomSIH/uHDh1+zD0/oB+BK48884+4SAAAAgAqryIF/2bJlpVkHAAAAAABwoSLfww8AAAAAACoOAj8AAAAAACZE4AcAAAAAwIQI/AAAAAAAmFCRAv9NN92k1NRUSdLMmTOVmZlZqkUBAAAAAICSKVLgP3z4sC5evChJmjFjhi5cuFCqRQEAAAAAgJIp0sfytWjRQg888IBuvfVWGYahl19+Wdddd12hfZ977jmXFggAAAAAAJxXpMAfFxen6dOna/369bJYLPrkk0/k5VXwpRaLhcAPAAAAAEA5UKTAf+ONN2rlypWSJA8PD23evFk1atQo1cIAAAAAAEDxFSnwX85ms5VGHQAAAAAAwIWcDvyS9OOPP2rhwoU6fPiwLBaLGjVqpAkTJuj66693dX0AAAAAAKAYivSU/st9+umnaty4sXbt2qVmzZqpadOm+uqrr9SkSRMlJCSURo0AAAAAAMBJTq/wP/XUU5o4caLmzp1boP3JJ59U9+7dXVYcAACu8PCvz+tUlVF6+NfnpfhKzg8wZJXriwIAAChlTq/wHz58WCNHjizQ/uCDD+q7775zSVEAAAAAAKBknA781atX1759+wq079u3jyf3AwAAAABQTjh9Sf+oUaM0evRo/fTTT2rfvr0sFou2b9+uF198UY8//nhp1AgAAAAAAJzkdOB/9tlnFRAQoPnz52vq1KmSpPDwcMXGxuqxxx5zeYEAAAAAAMB5Tgd+i8WiiRMnauLEicrIyJAkBQQEuLwwAAAAAABQfE4H/ssR9AEAAAAAKJ+cfmgfAAAAAAAo/wj8AAAAAACYEIEfAAAAAAATcirw5+TkqHPnzvrf//5XWvUAAAAAAAAXcCrwW61WHTx4UBaLpbTqAQAAAAAALuD0Jf3333+/li5dWhq1AAAAAAAAF3H6Y/kuXbqkf/3rX0pISFDr1q1VqVIlh/0LFixwWXEAAAAAAKB4nA78Bw8e1E033SRJBe7l51J/AAAAAADKB6cD/5YtW0qjDgAAAAAA4ELF/li+H374QZ9++qmysrIkSYZhuKwoAAAAAABQMk4H/rNnz6pr165q0KCBevfuraSkJEnSQw89pMcff9zlBQIAAAAAAOc5HfgnTpwoq9WqkydPyt/f394+aNAgbdiwwaXFAQAAAACA4nH6Hv6NGzfq008/Va1atRza69evrxMnTrisMAAAAAAAUHxOr/BfvHjRYWU/32+//SYfHx+XFAUAAAAAAErG6cDfsWNHvfXWW/Zti8Uim82ml156SZ07d3ZpcQAAAAAAoHicvqT/pZdeUnR0tL7++mtdunRJU6ZM0aFDh3Tu3Dl98cUXpVEjAAAAAABwktMr/I0bN9b+/ft18803q3v37rp48aIGDBigvXv36vrrry+NGgEAAAAAgJOcXuGXpNDQUM2YMcPVtQAAAAAAABcpVuBPTU3V0qVLdfjwYVksFjVq1EgPPPCAgoODXV0fAAAAAAAoBqcv6U9MTFTdunX16quvKjU1VefOndOrr76qunXrKjExsTRqBAAAAAAATnJ6hX/s2LEaOHCgXn/9dXl6ekqS8vLyNGbMGI0dO1YHDx50eZEAAAAAAMA5Tq/w//jjj3r88cftYV+SPD09NWnSJP34448uLQ4AAAAAABSP04H/pptu0uHDhwu0Hz58WC1atHBFTQAAAAAAoISKdEn//v377b9/7LHHNGHCBP3www9q166dJGnnzp167bXXNHfu3NKpEgAAAAAAOKVIgb9FixayWCwyDMPeNmXKlAL9hgwZokGDBrmuOgAAAAAAUCxFCvzHjh0r7ToAAAAAAIALFSnwR0ZGlnYdAAAAAADAhZz+WD5J+uWXX/TFF18oJSVFNpvNYd9jjz3mksIAAAAAAEDxOR34ly1bpkceeUTe3t6qWrWqLBaLfZ/FYiHwAwAAAABQDjgd+J977jk999xzmjp1qjw8nP5UPwAAAAAAUAacDvyZmZkaPHgwYR9AoUbG7XZ3CQAAAAAkOZ3aR44cqffee680agEAAAAAAC7i9Ar/nDlz1LdvX23YsEFRUVGyWq0O+xcsWOCy4gAAAAAAQPE4vcI/e/Zsffrppzpz5owOHDigvXv32r/27dvn1Fjbtm3T3/72N4WHh8tisWjt2rUO+w3DUGxsrMLDw+Xn56fo6GgdOnTIoU92drbGjx+vatWqqVKlSurXr59+/vlnZ08LAAAAAABTcTrwL1iwQP/+9791+PBhbd26VVu2bLF/ffbZZ06NdfHiRTVv3lyLFy8udP+8efO0YMECLV68WLt371ZoaKi6d++ujIwMe5+YmBitWbNGK1eu1Pbt23XhwgX17dtXeXl5zp4aAAAAAACm4fQl/T4+PurQoYNLDt6rVy/16tWr0H2GYWjhwoWaNm2aBgwYIElavny5QkJCFB8fr4cfflhpaWlaunSpVqxYoW7dukmS3n77bUVERGjTpk3q2bNnoWNnZ2crOzvbvp2eni5JysnJUU5OzjXrzu9TlL5AReKKue0lm6vKkc3Deu1OQBHkzyWbh1U5zv/TJ/H9HuUUP5PArJjbMKvL53ZZzG+LYRiGMy+YM2eOkpKS9Oqrr7q2EItFa9asUf/+/SVJP/30k66//np98803atmypb3fHXfcoaCgIC1fvlyfffaZunbtqnPnzqlKlSr2Ps2bN1f//v01Y8aMQo8VGxtb6L74+Hj5+/u79LwAAAAAAPizzMxMDRkyRGlpaapcuXKpHMPpZY5du3bps88+0/r169WkSZMCD+1bvXq1SwpLTk6WJIWEhDi0h4SE6MSJE/Y+3t7eDmE/v0/+6wszdepUTZo0yb6dnp6uiIgI9ejRo0hvdE5OjhISEtS9e/cC5w9UZK6Y2+Pe+cZl9Tz86/MuGwt/bTYPq365YYRq/hCn5mHF+I/de+JcXhPgCvxMArNibsOsLp/bWVlZpX48pwN/UFCQ/RL7smCxWBy2DcMo0PZn1+rj4+MjHx+fAu1Wq9WpbyjO9gcqipLM7VznHw1yRR42LuODa3nYcmRVrvMv5Hs9yjl+JoFZMbdhVlarVbm5xfiZxElOB/5ly5aVRh0FhIaGSvpjFT8sLMzenpKSYl/1Dw0N1aVLl5Samuqwyp+SkqL27duXSZ0AAAAAAJRHrluKc7G6desqNDRUCQkJ9rZLly4pMTHRHuZbtWolq9Xq0CcpKUkHDx4k8AMAAAAA/tKcXuGvW7fuVS+X/+mnn4o81oULF/TDDz/Yt48dO6Z9+/YpODhYtWvXVkxMjGbPnq369eurfv36mj17tvz9/TVkyBBJUmBgoEaOHKnHH39cVatWVXBwsCZPnqyoqCj7U/sBAAAAAPgrcjrwx8TEOGzn5ORo79692rBhg5544gmnxvr666/VuXNn+3b+g/SGDx+uuLg4TZkyRVlZWRozZoxSU1PVtm1bbdy4UQEBAfbXvPLKK/Ly8tLAgQOVlZWlrl27Ki4uTp6ens6eGgAAAAAApuF04J8wYUKh7a+99pq+/vprp8aKjo7W1T4V0GKxKDY2VrGxsVfs4+vrq0WLFmnRokVOHRsAAAAAADNz2T38vXr10n//+19XDQcAAAAAAErA6RX+K3n//fcVHBzsquEAmMD4M8+4uwQAAADgL8vpwN+yZUuHh/YZhqHk5GT9+uuvWrJkiUuLAwAAAAAAxeN04O/fv7/DtoeHh6pXr67o6Gg1bNjQVXUBAAAAAIAScDrwT58+vTTqAAAAAAAALuSyh/YBAAAAAIDyo8gr/B4eHg737hfGYrEoNze3xEUBAFCuxA8q+RhDVpV8DAAAACcUOfCvWbPmivu+/PJLLVq0SIZhuKQoAAAAAABQMkUO/HfccUeBtu+//15Tp07Vhx9+qKFDh+r55593aXEAAAAAAKB4inUP/+nTpzVq1Cg1a9ZMubm52rdvn5YvX67atWu7uj4AAAAAAFAMTgX+tLQ0Pfnkk7rhhht06NAhbd68WR9++KGaNm1aWvUBAAAAAIBiKPIl/fPmzdOLL76o0NBQvfvuu4Ve4g8AAAAAAMqHIgf+p556Sn5+frrhhhu0fPlyLV++vNB+q1evdllxAAAAAACgeIoc+O+///5rfiwfAADl3b5T50s8RouIoBKPAQAAUNqKHPjj4uJKsQwAAAAAAOBKxXpKPwAAAAAAKN8I/AAAAAAAmBCBHwAAAAAAEyryPfwAAOAPxXnw36K43Q7bS0e0cVE1AAAAhWOFHwAAAAAAEyLwAwAAAABgQgR+AAAAAABMiMAPAAAAAIAJEfgBAAAAADAhAj8AAAAAACZE4AcAAAAAwIQI/AAAAAAAmBCBHwAAAAAAEyLwAwAAAABgQgR+AAAAAABMiMAPAAAAAIAJEfgBAAAAADAhAj8AAAAAACZE4AcAAAAAwIQI/AAAAAAAmBCBHwAAAAAAEyLwAwAAAABgQgR+AAAAAABMyMvdBQAoP8a98416V/nj11z+PxAAAACo0PiJHgAAAAAAEyLwAwAAAABgQgR+AAAAAABMiMAPAAAAAIAJEfgBAAAAADAhAj8AAAAAACZE4AcAAAAAwIQI/AAAAAAAmBCBHwAAAAAAEyLwAwAAAABgQgR+AAAAAABMiMAPAAAAAIAJEfgBAAAAADAhAj8AAAAAACZE4AcAAAAAwIQI/AAAAAAAmJCXuwsAAOCvYPyZZxwb4oOcG2DIKpfVAgAA/hpY4QcAAAAAwIQI/AAAAAAAmBCX9AMoVIHLjwEAAABUKKzwAwAAAABgQgR+AAAAAABMiMAPAAAAAIAJEfgBAAAAADAhAj8AAAAAACZE4AcAAAAAwIQI/AAAAAAAmBCBHwAAAAAAEyLwAwAAAABgQgR+AAAAAABMiMAPAAAAAIAJEfgBAAAAADAhAj8AAAAAACZE4AcAAAAAwIQI/AAAAAAAmJCXuwsAAOCvaN+p8071XxS3u0Db0hFtXFQNAAAwo3K9wh8bGyuLxeLwFRoaat9vGIZiY2MVHh4uPz8/RUdH69ChQ26sGAAAAACA8qFcB35JatKkiZKSkuxfBw4csO+bN2+eFixYoMWLF2v37t0KDQ1V9+7dlZGR4caKAQAAAABwv3If+L28vBQaGmr/ql69uqQ/VvcXLlyoadOmacCAAWratKmWL1+uzMxMxcfHu7lqAAAAAADcq9zfw3/06FGFh4fLx8dHbdu21ezZs1WvXj0dO3ZMycnJ6tGjh72vj4+POnXqpC+//FIPP/zwFcfMzs5Wdna2fTs9PV2SlJOTo5ycnGvWlN+nKH2BisRLNvuvNg+rm6sBXCd/PlfkeZ3/9/Ny/DsEfiaBWTG3YVaXz+2ymN8WwzCMUj9KMX3yySfKzMxUgwYNdObMGb3wwgv6/vvvdejQIR05ckQdOnTQL7/8ovDwcPtrRo8erRMnTujTTz+94rixsbGaMWNGgfb4+Hj5+/uXyrkAAAAAAJAvMzNTQ4YMUVpamipXrlwqxyjXgf/PLl68qOuvv15TpkxRu3bt1KFDB50+fVphYWH2PqNGjdKpU6e0YcOGK45T2Ap/RESEfvvttyK90Tk5OUpISFD37t1ltVbc1SLgz2Le+Vo9qqRoY2oNjfx1lrvLAVzG5mHVLzeMUM0f4uRhq5irRW9Uf7ZA2+KhN7mhEpQn/EwCs2Juw6wun9tZWVmqVq1aqQb+cn9J/+UqVaqkqKgoHT16VP3795ckJScnOwT+lJQUhYSEXHUcHx8f+fj4FGi3Wq1OfUNxtj9Q3uX+32M9cuVRYUMRcDUetpwKO7dzC3nsDv8GIR8/k8CsmNswK6vVqtzc3FI/Trl/aN/lsrOzdfjwYYWFhalu3boKDQ1VQkKCff+lS5eUmJio9u3bu7FKAAAAAADcr1yv8E+ePFl/+9vfVLt2baWkpOiFF15Qenq6hg8fLovFopiYGM2ePVv169dX/fr1NXv2bPn7+2vIkCHuLh0AAAAAALcq14H/559/1r333qvffvtN1atXV7t27bRz505FRkZKkqZMmaKsrCyNGTNGqampatu2rTZu3KiAgAA3Vw4AQDkTP6hkrx+yyjV1AACAMlOuA//KlSuvut9isSg2NlaxsbFlUxAAAAAAABVEuQ78AADgD+PPPFOwMT6ozOsAAAAVR4V6aB8AAAAAACgaAj8AAAAAACZE4AcAAAAAwIQI/AAAAAAAmBAP7QPMqhgfwfXwrxd1qsooPfzr86VQEAAAAICyxAo/AAAAAAAmROAHAAAAAMCECPwAAAAAAJgQgR8AAAAAABMi8AMAAAAAYEIEfgAAAAAATIjADwAAAACACRH4AQAAAAAwIQI/AAAAAAAmROAHAAAAAMCECPwAAAAAAJgQgR8AAAAAABMi8AMAAAAAYEIEfgAAAAAATIjADwAAAACACRH4AQAAAAAwIQI/AAAAAAAm5OXuAgAAQPHsO3W+xGO0iAgq8RgAAKB8YoUfAAAAAAATIvADAAAAAGBCXNIPmMTIuN0O2+PPnHd+EA+ra4oBUGEU9baARX/6HnO5pSPauKgaAADgSqzwAwAAAABgQgR+AAAAAABMiMAPAAAAAIAJEfgBAAAAADAhAj8AAAAAACZE4AcAAAAAwIT4WD6gvIof5FT3Yn0MHwAAAADTYoUfAAAAAAATIvADAAAAAGBCXNIPAADKhpO3KhUwZJVr6gAA4C+CFX4AAAAAAEyIwA8AAAAAgAkR+AEAAAAAMCHu4QcAANc0/swzV94ZH1RmdQAAgKJjhR8AAAAAABNihR8oDcV4EvW+U+ddXwcAAACAvyxW+AEAAAAAMCECPwAAAAAAJkTgBwAAAADAhAj8AAAAAACYEA/tQ4U2Mm53icdYOqKNCyoBgL8uVzx0tEVEUInHAAAAjljhBwAAAADAhAj8AAAAAACYEIEfAAAAAAATIvADAAAAAGBCPLTvL+iv8KC78WeeKXrn+KCCbUNWuawWAAAAAHAHVvgBAAAAADAhAj8AAAAAACbEJf2AC/z5NonxZ867pxAAAAAA+D+s8AMAAAAAYEKs8AMAALfbd+r8NfssusZDZ8v7A2UBAChrrPADAAAAAGBCBH4AAAAAAEyIwA8AAAAAgAlxD7/ZxA+6ZpdrPUF+UcgLLioGAADXGX/mmat3iA+6+v4hq1xWCwAAFQGBHwAAmMK1Hvx3rYf+STz4DwBgLlzSDwAAAACACbHCDwAAUFRFuHXuqritAABQhljhBwAAAADAhFjhBwrj5ArOtR6ECACoGEZe4z7/kn6/5zkCAICyxAo/AAAAAAAmxAo/AAD4S7jmx/oBAGAyBH4AAAAUcK3bG8oKtzgAQPFxST8AAAAAACbECj/+8vadOu/uEgAAAADA5VjhBwAAAADAhFjhr2BK++OC/hijCA81ig+68r4hq0pWgBMficfH4QEAzOZq/9b/+d9om4dVajBKBxbeKQ9bTpHGXxTyQonqKxec/PjcAkr6swoAVBCs8AMAAAAAYEKs8MNtrrSCwao9AMCs+GhAAEBZMk3gX7JkiV566SUlJSWpSZMmWrhwoW677TZ3l2VaV33Q3Ys9izTGeNeUAgAA4BxuCQBcp6R/nyT+TpUiU1zSv2rVKsXExGjatGnau3evbrvtNvXq1UsnT550d2kAAAAAALiFKVb4FyxYoJEjR+qhhx6SJC1cuFCffvqpXn/9dc2ZM8fN1QEAAJQfFe22gn0vuruCP7SICHJ3CY7Kw1UK5aGGcuBaD9UuK0tHtHF3CS7jivfUTO9HSVT4wH/p0iXt2bNHTz31lEN7jx499OWXXxb6muzsbGVnZ9u309LSJEnnzp1TTs61n3Cbk5OjzMxMnT17VlartQTVO8+WlXHV/emXyqgQmJLNQ8rMzFT6JcnD5u5qANdhbsOsmNtl52ymcdnGWfcVku/yeorDFedQijW48+dtZ13r5/OyctZd87Kk80AqMBdc8Z667f24hsvn9u+//y5JMgwXvIdXYDFKc/QycPr0adWsWVNffPGF2rdvb2+fPXu2li9friNHjhR4TWxsrGbMmFGWZQIAAAAAUMCpU6dUq1atUhm7wq/w57NYLA7bhmEUaMs3depUTZo0yb5ts9l07tw5Va1a9YqvuVx6eroiIiJ06tQpVa5cuWSFA+UIcxtmxdyGWTG3YVbMbZjV5XM7ICBAGRkZCg8PL7XjVfjAX61aNXl6eio5OdmhPSUlRSEhIYW+xsfHRz4+Pg5tQUFBTh+7cuXKfAOCKTG3YVbMbZgVcxtmxdyGWeXP7cDAwFI9ToV/Sr+3t7datWqlhIQEh/aEhASHS/wBAAAAAPgrqfAr/JI0adIk3XfffWrdurVuueUW/fOf/9TJkyf1yCOPuLs0AAAAAADcwhSBf9CgQTp79qxmzpyppKQkNW3aVB9//LEiIyNL5Xg+Pj6aPn16gdsCgIqOuQ2zYm7DrJjbMCvmNsyqrOd2hX9KPwAAAAAAKKjC38MPAAAAAAAKIvADAAAAAGBCBH4AAAAAAEyIwA8AAAAAgAkR+J20ZMkS1a1bV76+vmrVqpU+//xzd5cEXNWcOXPUpk0bBQQEqEaNGurfv7+OHDni0McwDMXGxio8PFx+fn6Kjo7WoUOHHPpkZ2dr/PjxqlatmipVqqR+/frp559/LstTAa5ozpw5slgsiomJsbcxr1GR/fLLLxo2bJiqVq0qf39/tWjRQnv27LHvZ36jIsrNzdUzzzyjunXrys/PT/Xq1dPMmTNls9nsfZjbqAi2bdumv/3tbwoPD5fFYtHatWsd9rtqHqempuq+++5TYGCgAgMDdd999+n8+fNO1Urgd8KqVasUExOjadOmae/evbrtttvUq1cvnTx50t2lAVeUmJiosWPHaufOnUpISFBubq569Oihixcv2vvMmzdPCxYs0OLFi7V7926Fhoaqe/fuysjIsPeJiYnRmjVrtHLlSm3fvl0XLlxQ3759lZeX547TAux2796tf/7zn2rWrJlDO/MaFVVqaqo6dOggq9WqTz75RN99953mz5+voKAgex/mNyqiF198Uf/4xz+0ePFiHT58WPPmzdNLL72kRYsW2fswt1ERXLx4Uc2bN9fixYsL3e+qeTxkyBDt27dPGzZs0IYNG7Rv3z7dd999zhVroMhuvvlm45FHHnFoa9iwofHUU0+5qSLAeSkpKYYkIzEx0TAMw7DZbEZoaKgxd+5ce5/ff//dCAwMNP7xj38YhmEY58+fN6xWq7Fy5Up7n19++cXw8PAwNmzYULYnAFwmIyPDqF+/vpGQkGB06tTJmDBhgmEYzGtUbE8++aRx6623XnE/8xsVVZ8+fYwHH3zQoW3AgAHGsGHDDMNgbqNikmSsWbPGvu2qefzdd98ZkoydO3fa++zYscOQZHz//fdFro8V/iK6dOmS9uzZox49eji09+jRQ19++aWbqgKcl5aWJkkKDg6WJB07dkzJyckOc9vHx0edOnWyz+09e/YoJyfHoU94eLiaNm3K/IdbjR07Vn369FG3bt0c2pnXqMjWrVun1q1b65577lGNGjXUsmVLvfnmm/b9zG9UVLfeeqs2b96s//3vf5Kkb7/9Vtu3b1fv3r0lMbdhDq6axzt27FBgYKDatm1r79OuXTsFBgY6Nde9SnpCfxW//fab8vLyFBIS4tAeEhKi5ORkN1UFOMcwDE2aNEm33nqrmjZtKkn2+VvY3D5x4oS9j7e3t6pUqVKgD/Mf7rJy5Up988032r17d4F9zGtUZD/99JNef/11TZo0SU8//bR27dqlxx57TD4+Prr//vuZ36iwnnzySaWlpalhw4by9PRUXl6eZs2apXvvvVcS37thDq6ax8nJyapRo0aB8WvUqOHUXCfwO8lisThsG4ZRoA0or8aNG6f9+/dr+/btBfYVZ24z/+Eup06d0oQJE7Rx40b5+vpesR/zGhWRzWZT69atNXv2bElSy5YtdejQIb3++uu6//777f2Y36hoVq1apbffflvx8fFq0qSJ9u3bp5iYGIWHh2v48OH2fsxtmIEr5nFh/Z2d61zSX0TVqlWTp6dngf9NSUlJKfC/N0B5NH78eK1bt05btmxRrVq17O2hoaGSdNW5HRoaqkuXLik1NfWKfYCytGfPHqWkpKhVq1by8vKSl5eXEhMT9eqrr8rLy8s+L5nXqIjCwsLUuHFjh7ZGjRrZHxLM921UVE888YSeeuopDR48WFFRUbrvvvs0ceJEzZkzRxJzG+bgqnkcGhqqM2fOFBj/119/dWquE/iLyNvbW61atVJCQoJDe0JCgtq3b++mqoBrMwxD48aN0+rVq/XZZ5+pbt26Dvvr1q2r0NBQh7l96dIlJSYm2ud2q1atZLVaHfokJSXp4MGDzH+4RdeuXXXgwAHt27fP/tW6dWsNHTpU+/btU7169ZjXqLA6dOhQ4ONT//e//ykyMlIS37dRcWVmZsrDwzF+eHp62j+Wj7kNM3DVPL7llluUlpamXbt22ft89dVXSktLc26uF/35g1i5cqVhtVqNpUuXGt99950RExNjVKpUyTh+/Li7SwOu6NFHHzUCAwONrVu3GklJSfavzMxMe5+5c+cagYGBxurVq40DBw4Y9957rxEWFmakp6fb+zzyyCNGrVq1jE2bNhnffPON0aVLF6N58+ZGbm6uO04LKODyp/QbBvMaFdeuXbsMLy8vY9asWcbRo0eNd955x/D39zfefvttex/mNyqi4cOHGzVr1jTWr19vHDt2zFi9erVRrVo1Y8qUKfY+zG1UBBkZGcbevXuNvXv3GpKMBQsWGHv37jVOnDhhGIbr5vHtt99uNGvWzNixY4exY8cOIyoqyujbt69TtRL4nfTaa68ZkZGRhre3t3HTTTfZP9oMKK8kFfq1bNkyex+bzWZMnz7dCA0NNXx8fIyOHTsaBw4ccBgnKyvLGDdunBEcHGz4+fkZffv2NU6ePFnGZwNc2Z8DP/MaFdmHH35oNG3a1PDx8TEaNmxo/POf/3TYz/xGRZSenm5MmDDBqF27tuHr62vUq1fPmDZtmpGdnW3vw9xGRbBly5ZCf74ePny4YRium8dnz541hg4dagQEBBgBAQHG0KFDjdTUVKdqtRiGYRTjSgUAAAAAAFCOcQ8/AAAAAAAmROAHAAAAAMCECPwAAAAAAJgQgR8AAAAAABMi8AMAAAAAYEIEfgAAAAAATIjADwAAAACACRH4AQAAAAAwIQI/AABAIeLi4hQUFOTuMgAAKDYCPwAALjRixAhZLBZZLBZZrVaFhISoe/fu+ve//y2bzebu8spMUcNyeQnVderU0cKFC91dBgAALkXgBwDAxW6//XYlJSXp+PHj+uSTT9S5c2dNmDBBffv2VW5urrvLAwAAfxEEfgAAXMzHx0ehoaGqWbOmbrrpJj399NP64IMP9MknnyguLs7e7+TJk7rjjjt03XXXqXLlyho4cKDOnDnjMNa6devUunVr+fr6qlq1ahowYIB9n8Vi0dq1ax36BwUF2Y9x/PhxWSwW/ec//9Ftt90mPz8/tWnTRv/73/+0e/dutW7dWtddd51uv/12/frrrw7jLFu2TI0aNZKvr68aNmyoJUuW2Pflj7t69Wp17txZ/v7+at68uXbs2CFJ2rp1qx544AGlpaXZr3aIjY0t1nuZlpam0aNHq0aNGqpcubK6dOmib7/91r4/NjZWLVq00IoVK1SnTh0FBgZq8ODBysjIsPfJyMjQ0KFDValSJYWFhemVV15RdHS0YmJiJEnR0dE6ceKEJk6caK/3cp9++qkaNWpkf6+SkpKKdS4AAJQ1Aj8AAGWgS5cuat68uVavXi1JMgxD/fv317lz55SYmKiEhAT9+OOPGjRokP01H330kQYMGKA+ffpo79692rx5s1q3bu30sadPn65nnnlG33zzjby8vHTvvfdqypQp+vvf/67PP/9cP/74o5577jl7/zfffFPTpk3TrFmzdPjwYc2ePVvPPvusli9f7jDutGnTNHnyZO3bt08NGjTQvffeq9zcXLVv314LFy5U5cqVlZSUpKSkJE2ePNnpug3DUJ8+fZScnKyPP/5Ye/bs0U033aSuXbvq3Llz9n4//vij1q5dq/Xr12v9+vVKTEzU3Llz7fsnTZqkL774QuvWrVNCQoI+//xzffPNN/b9q1evVq1atTRz5kx7vfkyMzP18ssva8WKFdq2bZtOnjxZrHMBAMAdvNxdAAAAfxUNGzbU/v37JUmbNm3S/v37dezYMUVEREiSVqxYoSZNmmj37t1q06aNZs2apcGDB2vGjBn2MZo3b+70cSdPnqyePXtKkiZMmKB7771XmzdvVocOHSRJI0eOdLjy4Pnnn9f8+fPtVxPUrVtX3333nd544w0NHz7cYdw+ffpIkmbMmKEmTZrohx9+UMOGDRUYGCiLxaLQ0FCn6823ZcsWHThwQCkpKfLx8ZEkvfzyy1q7dq3ef/99jR49WpJks9kUFxengIAASdJ9992nzZs3a9asWcrIyNDy5csVHx+vrl27Svrj6oXw8HD7cYKDg+Xp6amAgIAC9ebk5Ogf//iHrr/+eknSuHHjNHPmzGKfEwAAZYnADwBAGTEMw365+OHDhxUREWEP+5LUuHFjBQUF6fDhw2rTpo327dunUaNGlfi4zZo1s/8+JCREkhQVFeXQlpKSIkn69ddfderUKY0cOdLh2Lm5uQoMDLziuGFhYZKklJQUNWzYsMQ1S9KePXt04cIFVa1a1aE9KytLP/74o327Tp069rCfX0v++fz000/KycnRzTffbN8fGBioG2+8sUg1+Pv728P+n8cGAKC8I/ADAFBGDh8+rLp160pyDP+Xu7zdz8/vquNZLBYZhuHQlpOTU6Cf1Wp1eE1hbfmfIJD/65tvvqm2bds6jOPp6XnNcV35SQQ2m01hYWHaunVrgX2XP9n/8jrya8mvI//9+fN7/ef37UoKG7uorwUAwN24hx8AgDLw2Wef6cCBA7rrrrsk/bGaf/LkSZ06dcre57vvvlNaWpoaNWok6Y8V9M2bN19xzOrVqzvcb3706FFlZmaWqM6QkBDVrFlTP/30k2644QaHr/z/rCgKb29v5eXllaiWm266ScnJyfLy8ipQS7Vq1Yo0xvXXXy+r1apdu3bZ29LT03X06FGX1wsAQHnDCj8AAC6WnZ2t5ORk5eXl6cyZM9qwYYPmzJmjvn376v7775ckdevWTc2aNdPQoUO1cOFC5ebmasyYMerUqZP9wXzTp09X165ddf3112vw4MHKzc3VJ598oilTpkj640GAixcvVrt27WSz2fTkk08WWJEujtjYWD322GOqXLmyevXqpezsbH399ddKTU3VpEmTijRGnTp1dOHCBW3evFnNmzeXv7+//P39C+2bl5enffv2ObR5e3urW7duuuWWW9S/f3+9+OKLuvHGG3X69Gl9/PHH6t+/f5EeYBgQEKDhw4friSeeUHBwsGrUqKHp06fLw8PDYdW/Tp062rZtmwYPHiwfH58i/4cCAADlGSv8AAC42IYNGxQWFqY6dero9ttv15YtW/Tqq6/qgw8+sF8Wn/+RelWqVFHHjh3VrVs31atXT6tWrbKPEx0drffee0/r1q1TixYt1KVLF3311Vf2/fPnz1dERIQ6duyoIUOGaPLkyVcM1c546KGH9K9//UtxcXGKiopSp06dFBcX59QKf/v27fXII49o0KBBql69uubNm3fFvhcuXFDLli0dvnr37i2LxaKPP/5YHTt21IMPPqgGDRpo8ODBOn78uP1ZBEWxYMEC3XLLLerbt6+6deumDh062D9yMN/MmTN1/PhxXX/99apevXqRxwYAoDyzGNyIBgAA/kIuXryomjVrav78+Ro5cqS7ywEAoNRwST8AADC1vXv36vvvv9fNN9+stLQ0+8fq3XHHHW6uDACA0kXgBwAApvfyyy/ryJEj8vb2VqtWrfT5559znz4AwPS4pB8AAAAAABPioX0AAAAAAJgQgR8AAAAAABMi8AMAAAAAYEIEfgAAAAAATIjADwAAAACACRH4AQAAAAAwIQI/AAAAAAAmROAHAAAAAMCE/h96AxT/rt/y2AAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Term Frequencies\n",
    "all_tokens = [token for text in train_texts for token in preprocess_text(text)]\n",
    "common_tokens = Counter(all_tokens).most_common(10)\n",
    "print(\"\\nMost common words across the training set:\")\n",
    "for token, freq in common_tokens:\n",
    "    print(f\"{token}: {freq}\")\n",
    "\n",
    "# Document Length Distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(train_doc_lengths, bins=50, alpha=0.7, label='Train')\n",
    "plt.hist(test_doc_lengths, bins=50, alpha=0.7, label='Test')\n",
    "plt.title('Document Length Distribution')\n",
    "plt.xlabel('Document Length')\n",
    "plt.ylabel('Number of Documents')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kowsh\\anaconda3\\envs\\DatasScienceProgrammingwithPy\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# The Tfidf Vectorizer will use our custom tokenizer/preprocesser\n",
    "vectorizer = TfidfVectorizer(tokenizer=preprocess_text,max_features=2500) #using the preprocess_text function defined earlier\n",
    "train_dtm = vectorizer.fit_transform(train_texts)\n",
    "test_dtm = vectorizer.transform(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500\n",
      "(2500, 2500)\n",
      "2500\n",
      "(2500, 2500)\n"
     ]
    }
   ],
   "source": [
    "print(len(train_labels)) #train_labels is equivalent to y_train\n",
    "print(train_dtm.shape) \n",
    "print(len(test_labels)) #test_labels is equivalent to y_test\n",
    "print(test_dtm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Statistical Tools Used and Problem Solving Approach**\n",
    "\n",
    "I have implemented a diverse set of classification algorithms from scikit learn, xgboost and pytorch (for neural nets) to tackle our author classification task. We have leveraged traditional machine learning techniques (like Logistic Regression and Naive Bayes), few advanced tree based methods (like Random Forest and XGBoost), ensemble algorithms(to combine the strength of individual models) and deep learning techniques like neural networks. \n",
    "\n",
    "1. Logistic Regression : I have used this algorithm to analyze how well a linear binary classification model performs in a multi-class classification scenario.\n",
    "\n",
    "2. Multinomial Naive Bayes : I have used Multinomial Naive Bayes, a probability-based multi-class classifier that assumes independence between features, for author classification. This algorithm is known to be well-suited for text classification tasks where features are often word counts/TF-IDFs. \n",
    "\n",
    "3. Random Forest : Random Forest creates multiple decision trees during training and combines their predictions to make a final decision. Since it handles non-linearity well and can perform with high-dimensional data, we expect it to yield competitive results in our case.  \n",
    "\n",
    "4. XG Boosting : XGBoost (Extreme Gradient Boosting) builds a strong predictive model by adding up the predictions of multiple weak models. It is basically a sequential training process where each new model works on reducing the residuals of the previous model. \n",
    "\n",
    "5. Ensemble Algorithm : Ensemble algorithms involve combining the predictions of multiple strong models to generate a more accurate and robust final prediction. We have combined the predictions of Multinomial Naive Bayes and Random Forest - as these gave the highest accuracy at an individual level for our author clasification task. In this case, I used soft voting with the Voting Classifier from sklearn, where I basically take a weighted average of the class probabilites from each of the two models and predict the class with the highest weighted average for a particular document. \n",
    "\n",
    "6. Neural Networks : I have used the PyTorch Library to build and train our neural networks for author classification. I also experimented with different network design parameters, activation functions, hidden layers and model learning optimization techniques. But, due to limited system processing capabilities, we could only optimize the performance of neural networks to some extent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note :** \n",
    "1. I have performed 5-fold cross validation on all the traditional and tree-based machine learning algorithms using GridSearchCV function from the model selection module of scikit-learn. We use cross validation to select the best hyper parameters and fit the model on the entire training dataset using the selected best hyperparameters. Model evaluation is then performed on this best model using the test data set. \n",
    "\n",
    "2. In xgboost, I did not perform cross validation or iterative hyper-parameter tuning as the system could not take up that level of computational complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Parameter Tuning Using 5-fold Cross Validation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Penalty Parameters  Accuracy(%)\n",
      "0       {'C': 0.001}         2.00\n",
      "1        {'C': 0.01}         2.00\n",
      "2         {'C': 0.1}        10.52\n",
      "3          {'C': 10}        63.92\n",
      "4         {'C': 100}        65.28\n",
      "5         {'C': 200}        65.36\n",
      "6         {'C': 300}        65.32\n",
      "7        {'C': 1000}        65.60\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "#creating our logistic regression model with lasso regularization and liblinear algorithm \n",
    "logis_text = LogisticRegression(penalty='l1',solver='liblinear')\n",
    "\n",
    "#parameter tuning using 5-fold cross validation\n",
    "param_grid={'C':[0.001,0.01,0.1,10,100,200,300,1000]}\n",
    "gridsearch=GridSearchCV(logis_text,param_grid,cv=5,scoring=\"accuracy\")\n",
    "\n",
    "gridsearch.fit(train_dtm,train_labels)\n",
    "\n",
    "best_params_list =[]\n",
    "best_scores_list=[]\n",
    "\n",
    "for param, score in zip(gridsearch.cv_results_['params'],gridsearch.cv_results_['mean_test_score']):\n",
    "    best_params_list.append(param)\n",
    "    best_scores_list.append(round((score*100),3))\n",
    "\n",
    "cvresults_df=pd.DataFrame({\"Penalty Parameters\": best_params_list,\"Accuracy(%)\": best_scores_list})\n",
    "print(cvresults_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Fitting the model with best parameter and training data + Model Evaluation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy from Logistic Regression : 61.00%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#fitting the model with the chosen parameter again on the entire training data and measuring accuracy\n",
    "best_model = LogisticRegression(penalty='l1', solver='liblinear', C=1000)\n",
    "\n",
    "best_model.fit(train_dtm,train_labels)\n",
    "\n",
    "#making predictions for the documents in the test file\n",
    "predictions = best_model.predict(test_dtm)\n",
    "\n",
    "#calculating accuracy\n",
    "acc_logis=(round(accuracy_score(test_labels,predictions),2))*100\n",
    "print(f\"Test Set Accuracy from Logistic Regression : {acc_logis:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multinomial Naive Bayes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Parameter Tuning Using 5-fold Cross Validation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Alpha Parameters  Accuracy(%)\n",
      "0  {'alpha': 0.001}        68.16\n",
      "1   {'alpha': 0.01}        69.04\n",
      "2    {'alpha': 0.1}        68.44\n",
      "3    {'alpha': 1.0}        65.48\n",
      "4   {'alpha': 10.0}        62.40\n",
      "5  {'alpha': 100.0}        60.92\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#creating the Naive Bayes model\n",
    "naive_bayes = MultinomialNB()\n",
    "\n",
    "#parameter tuning using 5-fold cross validation\n",
    "param_grid = {'alpha': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]}\n",
    "grid_search = GridSearchCV(naive_bayes, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "grid_search.fit(train_dtm, train_labels)\n",
    "\n",
    "best_params_list = []\n",
    "best_scores_list = []\n",
    "\n",
    "for param, score in zip(grid_search.cv_results_['params'], grid_search.cv_results_['mean_test_score']):\n",
    "    best_params_list.append(param)\n",
    "    best_scores_list.append(round((score*100),3))\n",
    "\n",
    "cvresults_df = pd.DataFrame({\"Alpha Parameters\": best_params_list, \"Accuracy(%)\": best_scores_list})\n",
    "print(cvresults_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Fitting the model with best parameter and training data + Model Evaluation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy from Multinomial Naive Bayes : 64.76%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#fitting the model with the chosen parameter again on the entire training data and measuring accuracy\n",
    "best_model = MultinomialNB(alpha=0.01)\n",
    "\n",
    "best_model.fit(train_dtm,train_labels)\n",
    "\n",
    "#making predictions for the documents in the test file\n",
    "predictions = best_model.predict(test_dtm)\n",
    "\n",
    "#calculating accuracy\n",
    "acc_nb=round((accuracy_score(test_labels,predictions)*100),2)\n",
    "print(f\"Test Set Accuracy from Multinomial Naive Bayes : {acc_nb:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Performing hyper parameter tuning using 5-fold cross validation* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           Parameters  Accuracy(%)\n",
      "0   {'max_depth': 5, 'min_samples_split': 2, 'n_es...        47.36\n",
      "1   {'max_depth': 5, 'min_samples_split': 2, 'n_es...        50.72\n",
      "2   {'max_depth': 5, 'min_samples_split': 2, 'n_es...        53.96\n",
      "3   {'max_depth': 5, 'min_samples_split': 5, 'n_es...        48.28\n",
      "4   {'max_depth': 5, 'min_samples_split': 5, 'n_es...        50.20\n",
      "5   {'max_depth': 5, 'min_samples_split': 5, 'n_es...        52.76\n",
      "6   {'max_depth': 5, 'min_samples_split': 10, 'n_e...        46.96\n",
      "7   {'max_depth': 5, 'min_samples_split': 10, 'n_e...        51.32\n",
      "8   {'max_depth': 5, 'min_samples_split': 10, 'n_e...        52.88\n",
      "9   {'max_depth': 10, 'min_samples_split': 2, 'n_e...        55.72\n",
      "10  {'max_depth': 10, 'min_samples_split': 2, 'n_e...        59.44\n",
      "11  {'max_depth': 10, 'min_samples_split': 2, 'n_e...        61.76\n",
      "12  {'max_depth': 10, 'min_samples_split': 5, 'n_e...        56.52\n",
      "13  {'max_depth': 10, 'min_samples_split': 5, 'n_e...        58.64\n",
      "14  {'max_depth': 10, 'min_samples_split': 5, 'n_e...        61.56\n",
      "15  {'max_depth': 10, 'min_samples_split': 10, 'n_...        54.84\n",
      "16  {'max_depth': 10, 'min_samples_split': 10, 'n_...        57.60\n",
      "17  {'max_depth': 10, 'min_samples_split': 10, 'n_...        61.08\n",
      "18  {'max_depth': 20, 'min_samples_split': 2, 'n_e...        62.96\n",
      "19  {'max_depth': 20, 'min_samples_split': 2, 'n_e...        64.96\n",
      "20  {'max_depth': 20, 'min_samples_split': 2, 'n_e...        67.12\n",
      "21  {'max_depth': 20, 'min_samples_split': 5, 'n_e...        63.92\n",
      "22  {'max_depth': 20, 'min_samples_split': 5, 'n_e...        65.20\n",
      "23  {'max_depth': 20, 'min_samples_split': 5, 'n_e...        66.44\n",
      "24  {'max_depth': 20, 'min_samples_split': 10, 'n_...        61.20\n",
      "25  {'max_depth': 20, 'min_samples_split': 10, 'n_...        65.04\n",
      "26  {'max_depth': 20, 'min_samples_split': 10, 'n_...        67.36\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create Random Forest model\n",
    "random_forest = RandomForestClassifier()\n",
    "\n",
    "# Parameter tuning using 5-fold cross validation\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "grid_search = GridSearchCV(random_forest, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "grid_search.fit(train_dtm, train_labels)\n",
    "\n",
    "best_params_list = []\n",
    "best_scores_list = []\n",
    "\n",
    "for param, score in zip(grid_search.cv_results_['params'], grid_search.cv_results_['mean_test_score']):\n",
    "    best_params_list.append(str(param))\n",
    "    best_scores_list.append(round((score*100),3))\n",
    "\n",
    "cvresults_df = pd.DataFrame({\"Parameters\": best_params_list, \"Accuracy(%)\": best_scores_list})\n",
    "print(cvresults_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Fitting the model with the best hyperparameters and training data + Model Evaluation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy from Random Forest : 60.8%\n"
     ]
    }
   ],
   "source": [
    "#fitting the model with the chosen parameter again on the entire training data and measuring accuracy\n",
    "best_model = RandomForestClassifier(n_estimators=200,max_depth=20,min_samples_split=10)\n",
    "\n",
    "best_model.fit(train_dtm,train_labels)\n",
    "\n",
    "#making predictions for the documents in the test file\n",
    "predictions = best_model.predict(test_dtm)\n",
    "\n",
    "#calculating accuracy\n",
    "acc_rf=round((accuracy_score(test_labels,predictions)*100),2)\n",
    "print(f\"Test Set Accuracy from Random Forest : {acc_rf}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XG (Extra Gradient) Boosting : Model Fitting + Model Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy from XG Boosting: 52.20%\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "boost_model=xgb.XGBClassifier(n_estimators=200, learning_rate=0.001, max_depth=3)\n",
    "\n",
    "boost_model.fit(xtrain_tensor,ytrain_tensor)\n",
    "\n",
    "pred=boost_model.predict(xtest_tensor)\n",
    "\n",
    "boost_acc=(accuracy_score(ytest_tensor,pred))*100\n",
    "\n",
    "print(f\"Test Accuracy from XG Boosting: {boost_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ensemble Algorithm : Multinomial Naive Bayes + Random Forest**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Part 1 : For Individual Models - Hyper Parameter Tuning with 5-fold CV + Model Fitting on Training Data with best hyperparamers*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Part 2 : Creating Ensemble of Individual Models using Voting Classifier + Fitting Ensemble Model on Training Data + Model Evaluation using Test Set*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble (Random Forest and Naive Bayes) Accuracy: 66.40%\n"
     ]
    }
   ],
   "source": [
    "#Ensemble of Naive Bayes and Random Forest\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "#creating the individual models\n",
    "naive_bayes = MultinomialNB()\n",
    "random_forest = RandomForestClassifier()\n",
    "\n",
    "#performing hyper parameter tuning\n",
    "param_grid_nb = {'alpha': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]}\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "#performing 5-fold cross validation for naive bayes and random forest\n",
    "grid_search_nb = GridSearchCV(naive_bayes, param_grid_nb, cv=5, scoring='accuracy')\n",
    "grid_search_rf = GridSearchCV(random_forest, param_grid_rf, cv=5, scoring='accuracy')\n",
    "\n",
    "#fitting the individual models\n",
    "grid_search_nb.fit(train_dtm, train_labels)\n",
    "grid_search_rf.fit(train_dtm, train_labels)\n",
    "\n",
    "#defining the ensemble model with voting classifier based on soft voting for calculating weighted probabilities for each class\n",
    "ensemble = VotingClassifier(\n",
    "    estimators=[('nb', grid_search_nb.best_estimator_), ('rf', grid_search_rf.best_estimator_)],\n",
    "    voting='soft' \n",
    ")\n",
    "\n",
    "#fitting the ensemble model\n",
    "ensemble.fit(train_dtm, train_labels)\n",
    "\n",
    "#making predictions for test data based on ensemble model\n",
    "ensemble_predictions = ensemble.predict(test_dtm)\n",
    "\n",
    "#calculating ensemble accuracy\n",
    "ensemble_accuracy = round((((ensemble_predictions == test_labels).sum() / len(test_labels))*100),3)\n",
    "print(f\"Ensemble (Random Forest and Naive Bayes) Accuracy: {ensemble_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deep Learning : Neural Network Implementation for Author Classification Problem**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Creating the neural network design - input/ouput layers, hidden layers and activation functions*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have designed the neural network to have 3 linear layers. Our first fully connected layer takes 2500 input neurons(all the features from the training set) and outputs 5000 neurons. The output from the first layer are then passed through a relu activation layer, which introduces non-linearity into the network, enabling the networks to learn the complex relationships in data. The third layer takes 5000 neurons as input and outputs 10,000 neurons. The outputs from the third layer are then passed on to the fourth layers which converts these 10,000 neurons into raw scores for 50 author classes. \n",
    "\n",
    "I have not created a soft max activation layer here because we are using CrossEntropyLoss from nn module as our loss function. CrossEntropyLoss function internally applies the soft max activation to our network output and then computes the cross entropy loss - the negative log likelihood loss using the actual author labels in the y tensor. Cross entropy loss basically quantfies the difference between the predicted labels and actual labels and our optimisation algorithm aims to minimise this loss.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "#constructing a classification neural network\n",
    "class AuthorClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AuthorClassifier, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer3 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.layer5 = nn.Linear(hidden_size2, num_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer5(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have used label encoder from scikit-learn to convert the categorical author labels of both test and train datasets to numerical labels. We also had to use .LongTensor for the author label lists to ensure that these labels are treated as integers and not floating point numbers. The tfidf vectors are converted to torch tensors, as our inputs need to be in a datatype that our neural network recognizes and understands. \n",
    "\n",
    "Note : I also tried to use one hot encoding to convert the labels to dummy variables. But, it was making the problem way more complex and we weren't able to trouble shoot it. But, this is something I have included in the way forward buckets of things that could be improved upon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#converting author names to numerical labels\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(train_labels)\n",
    "\n",
    "train_label_indices = label_encoder.transform(train_labels)\n",
    "test_label_indices = label_encoder.transform(test_labels)\n",
    "\n",
    "xtrain_tensor = torch.Tensor(train_dtm.toarray())\n",
    "ytrain_tensor = torch.LongTensor(train_label_indices)  \n",
    "xtest_tensor = torch.Tensor(test_dtm.toarray())\n",
    "ytest_tensor = torch.LongTensor(test_label_indices)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "input_size=train_dtm.shape[1]\n",
    "hidden_size1=input_size*2\n",
    "hidden_size2=hidden_size1*2\n",
    "num_class=50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I implemented the training loop for our Author Classifier neural netwok using a learning rate (step size) of 0.001 and 10 epochs. The loops iterates over 10 epochs, performs forward and backward passes and updates the model parameters using Adam optimizer based on the gradients computed during back propagation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 3.9121\n",
      "Epoch [2/10], Loss: 3.8027\n",
      "Epoch [3/10], Loss: 3.5997\n",
      "Epoch [4/10], Loss: 3.2317\n",
      "Epoch [5/10], Loss: 2.6848\n",
      "Epoch [6/10], Loss: 2.0439\n",
      "Epoch [7/10], Loss: 1.4490\n",
      "Epoch [8/10], Loss: 1.0127\n",
      "Epoch [9/10], Loss: 0.7314\n",
      "Epoch [10/10], Loss: 0.5463\n"
     ]
    }
   ],
   "source": [
    "\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "model = AuthorClassifier()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(xtrain_tensor)\n",
    "    loss = criterion(outputs, ytrain_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the author classifier network, I am evaluating how well the network performs on unseen test data in this phase. The network generates 50 raw logit scores for each test document which are then converted to class probabilities using the soft max function. Each row of the output of softmax function contains 50 class probabilities for each test document. I then select the class index with highest probability for each document and store it in predicted_classes. But, these will be integer labels and will be difficult to interpret. Hence, I used inverse tranform function from label_encoder to convert these numerical author labels to the corresponding predicted categorical author labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(xtest_tensor)  #xtest_tensor contains TF-IDF matrix for c50 test\n",
    "    predicted_probs = F.softmax(test_outputs, dim=1)  #applying softmax to get class probabilities for each of the 50 authors\n",
    "    predicted_classes = predicted_probs.argmax(dim=1)  #getting the predicted class (author) for each document\n",
    "\n",
    "#converting predicted_classes tensor to a NumPy array and inverse transform to get actual author labels\n",
    "predicted_author_labels = label_encoder.inverse_transform(predicted_classes.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Calculating the classification accuracy of neural network*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy from Neural Nets: 60.88%\n"
     ]
    }
   ],
   "source": [
    "#calculating accuracy\n",
    "correct_predictions = (predicted_classes.numpy() == ytest_tensor.numpy()).sum().item()\n",
    "total_examples = len(ytest_tensor)\n",
    "acc_nn = round((correct_predictions / total_examples)*100,2)\n",
    "\n",
    "print(f\"Test Accuracy from Neural Nets: {acc_nn:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_4ad82_row4_col0, #T_4ad82_row4_col1 {\n",
       "  background-color: yellow;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_4ad82\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_4ad82_level0_col0\" class=\"col_heading level0 col0\" >ML Model</th>\n",
       "      <th id=\"T_4ad82_level0_col1\" class=\"col_heading level0 col1\" >Test Set Accuracy(%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_4ad82_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_4ad82_row0_col0\" class=\"data row0 col0\" >Logistic Regression</td>\n",
       "      <td id=\"T_4ad82_row0_col1\" class=\"data row0 col1\" >61.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4ad82_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_4ad82_row1_col0\" class=\"data row1 col0\" >Multinomial Naive Bayes</td>\n",
       "      <td id=\"T_4ad82_row1_col1\" class=\"data row1 col1\" >64.760000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4ad82_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_4ad82_row2_col0\" class=\"data row2 col0\" >Random Forest</td>\n",
       "      <td id=\"T_4ad82_row2_col1\" class=\"data row2 col1\" >60.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4ad82_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_4ad82_row3_col0\" class=\"data row3 col0\" >XGBoosting</td>\n",
       "      <td id=\"T_4ad82_row3_col1\" class=\"data row3 col1\" >52.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4ad82_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_4ad82_row4_col0\" class=\"data row4 col0\" >Ensemble(Random Forest + Naive Bayes)</td>\n",
       "      <td id=\"T_4ad82_row4_col1\" class=\"data row4 col1\" >66.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4ad82_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_4ad82_row5_col0\" class=\"data row5 col0\" >Neural Networks</td>\n",
       "      <td id=\"T_4ad82_row5_col1\" class=\"data row5 col1\" >60.880000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x21d0c6fb460>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_summary=pd.DataFrame({\"ML Model\":[\"Logistic Regression\",\"Multinomial Naive Bayes\",\"Random Forest\",\"XGBoosting\",\"Ensemble(Random Forest + Naive Bayes)\",\"Neural Networks\"],\n",
    "\"Test Set Accuracy(%)\":[acc_logis,acc_nb,acc_rf,boost_acc,ensemble_accuracy,acc_nn]})\n",
    "\n",
    "#formatting the table\n",
    "\n",
    "def highlight(row):\n",
    "    if row.name==4:\n",
    "        return [\"background-color : yellow\"]* len(row)\n",
    "    return [\"\"]*len(row)\n",
    "\n",
    "formatted_results_summary = results_summary.style.apply(highlight,axis=1)\n",
    "formatted_results_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the table given above, I have furnished the test set accuracy percentages we could achieve from each of the 6 ML algorithms we implemented. \n",
    "\n",
    "For the author classification problem, the ensemble of Random Forest and Multinomal Naive Bayes gave the best test set accuracy of 66.4% - this means that the ensemble algorithm correctly classified 1660 out of the 2500 test documents with the correct authors.  \n",
    "\n",
    "I did not create an ensemble of logistic and multinomial naive bayes, because I wanted to include a model with complementing strengths - for example, Multinomial Naive Bayes excels in classifying documents based on word counts and tf-idf frequecies, whereas Random Forest due its ensemble learning will help us understand the complex patterns between the features within/across documents. Thus, I aimed to get the combine the strength of diverse models in order to make accurate and robust predictions for our author classification problem. \n",
    "\n",
    "Lack of system processing capabilities to fine tune the neural network optimization was one of the main reasons why I could not boost the performance of our neural nets further. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Way Forward**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While 66.4% is the best accuracy I could achieve in this project, I aim to fine tune our models further and overcome some of the roadblocks we faced during the implementation phase. Some of the action items in our 'work in progress' pipeline are listed below. \n",
    "\n",
    "1. Exploring optimization of network design parameters (no. of hidden layers, input and output sizes, activation functions..etc ) alongside network learning parameters (epochs and learning schedules) to boost the performance of neural networks. \n",
    "\n",
    "2. Exploring the impact of different kinds of encoding for author labels (label, one hot...etc) on the performance of both ensemble learning and deep learning algorithms. \n",
    "\n",
    "3. Considering a subset based implementation, where we check how well our model performs when we test only for say, 10,20,30 authors instead of 50 authors all at once. Does the accuracy of the model differ based on the subset size of authors? \n",
    "\n",
    "4. Expanding the horizons and understanding the workings of recurrent neural networks such as LSTM (Long Short Term Memory Networks) which seem to have given success in author classification problems as per research papers. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DatasScienceProgrammingwithPy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
